[
    {
        "pk": 1, 
        "model": "systems.SystemVersion", 
        "fields": {
            "support_systemarchitecture": true, 
            "support_logging": true, 
            "creator": "nbadam@andrew.cmu.edu", 
            "description_checkpoints": "In the event of a crash, the database starts of from where it was left, the OS takes care of writing data to disk and the database here doesn't need to take any snapshots. The on-disk representation is similar to the in-memory representation, there is no provision for compressing the data, due the memory map constraints.", 
            "support_datamodel": true, 
            "description_concurrencycontrol": "Locking overhead avoided by using MVCC, readers don't block at all and writers don't block readers. Deleted versions are reclaimed by the free space management module of LMDB (essentially stored into a B+ tree for later use).", 
            "support_joins": false, 
            "support_views": false, 
            "support_storagemodel": true, 
            "developer": "{Howard Chu}", 
            "description_querycompilation": "", 
            "description_logging": "No logging procedures are implemented here, using copy-on-write semantics (with shadow paging) provides durability without any need for logging. Shadow paging allows new writes to a different location and not directly replace the existing pages, hence avoids data-corruption. Also the shadow page reference update is atomic, hence avoids need for logging.", 
            "support_checkpoints": false, 
            "support_storedprocedures": false, 
            "support_isolationlevels": true, 
            "system": 1, 
            "description_storagemodel": "They use a memory-map to store the database with copy-on-write semantics, hence no specific storage model but the semantics are left to the operating system. The on-disk representation is similar to the memory representation of the database.", 
            "description_systemarchitecture": "LMDB uses shared-memory model i.e. it handles the memory as a single address space and all the threads access this in parallel. It uses copy-on-write semantics.", 
            "written_in": [
                5
            ], 
            "support_querycompilation": false, 
            "website": "{www.symas.com/mdb/}", 
            "support_storagearchitecture": true, 
            "description_indexes": "LMDB uses a modified design of Append-only B+ Tree and it uses 2 B+ trees : one for maintaining the regular user data pages and one for maintaining the free pages obtained after deletes. LMDB is optimized for short read transactions, long lived read transactions will keep older pages longer in the table and hence blocks write operations. If the workload has too many writes along with long lived read transactions, the performance would be very low.", 
            "description": "LMBD is a light-weight, high-speed embedded database for key-value data and it uses B+ trees to manage data. LMDB is inspired from Berkeley DB, essentially LMDB is made light-weight by removing and rewriting most of the BDB code. Few features in Berkeley DB like the 2-level locking(deadlocks and overhead avoided) and the additional 2-level caching (DB cache and backend cache in addition to file system cache) have been discarded in LMDB. LMDB is fully transactional and it complies with the ACID properties. The key features of LMDB is that it uses a single-level store, i.e, it uses **memory map** to store the database, which means that the OS is responsible for managing the pages (like caching frequently uses pages), this also means that the database can be larger than the memory and can grow upto the size of the virtual memory. Given that this embedded database system relinquishes most of the control of what to keep in memory and what to flush to disk, larger than memory databases may not perform well for certain workloads. MVCC concurrency model is enforced using a single writer(serialized) and the copy-on-write semantics to handle writes to the database. Readers don't block writers and writers don't block readers, and since there is single writer to the database there won't be any deadlocks. LMDB encourages read-only memory map, although write maps are supported in fully in-memory databases. LMDB maintains only two versions of data, i.e. once committed all the previous verions are discarded. A free list (another B+ tree apart from the one for the user pages) is maintained to track and reuse freed pages instead of going for new pages everytime, hence keeping the overall size usage optimized.", 
            "description_storedprocedures": "", 
            "description_views": "", 
            "description_queryinterface": "LMDB has no SQL layer but applications can directly access the database using API calls provided by LMDB. API support is not just in C but many wrappers for other languages have been developed by open-source contributors. All key-value store operations can be performed using these API calls.", 
            "description_queryexecution": "There is no query planning or query execution options as this is an embedded database, since we operate at individual key level, the closest we can classify it is under tuple-at-a-time. The user can program custom querying models on top this embeddded database, which can support other query execution options.", 
            "support_indexes": true, 
            "support_queryexecution": true, 
            "description_datamodel": "This embedded database is a key-value in the backend, which is stored in the memory-map. The keys are indexed in a B+ tree. LMDB provides transactional guarantees on top of this key-value store. It is not a relational database.", 
            "support_concurrencycontrol": true, 
            "description_foreignkeys": "", 
            "description_storagearchitecture": "LMDB uses mmap, hence it reliquishes most of the caching control to the OS. Memory map allows zero-copies for read/write and no additional buffers for the transaction control. Supports larger-than memory databases, it is bounded by the size of the virtual memory since they use a memory map.", 
            "name": "LMDB (Lightning Memory-mapped Database)", 
            "license": [], 
            "created": "2016-07-04 19:37:30", 
            "oses": [
                1, 
                4, 
                5, 
                7, 
                9, 
                10, 
                11, 
                13, 
                14, 
                16, 
                18
            ], 
            "description_joins": "", 
            "support_queryinterface": true, 
            "description_isolationlevels": "LMDB provides Serializable isolation with MVCC, this is possible because of the single-writer semantics. Only a single write transaction can can be alive at a single point of time, hence no races among multiple writers modifying the database.", 
            "support_foreignkeys": false, 
            "history": "LMDB is currently used in OpenLDAP (Berkeley DB was previously used since 1999). OpenLDAP is an open-source implementation of the Lightweight Directory Access Protocol (LDAP). As mentioned in the description, LMDB was mainly inspired from Berkeley DB, and discards many heavy modules from BDB. The object code size is around 40 KB (which is very light), can comfortably fit into modern CPU caches. The code for LMDB is mainly developed and maintained by the Symas Corporation."
        }
    }, 
    {
        "pk": 2, 
        "model": "systems.SystemVersion", 
        "fields": {
            "support_systemarchitecture": true, 
            "support_logging": true, 
            "creator": "abj1andrew.cmu.edu", 
            "description_checkpoints": "Snapshots/Checkpoints in Cassandra can be taken by using the nodetool command. By default, this command takes per-node snapshots, but can be used to take global snapshots by using a parallel ssh utility like pssh. Automatic incremental backups are disabled by default.", 
            "support_datamodel": true, 
            "description_concurrencycontrol": "Cassandra does not support RDBMS ACID transactions spanning multiple rows/tables. It also does not roll back when a write succeeds on one replica, but fails on other replicas. As far as concurrent reads and writes are concerned, Cassandra simply performs an isolated/atomic replacement of rows within mem-tables (the in-memory structure where all writes are buffered). These are implemented using Optimistic Concurrency Control. However, under high contention on a single partition, Cassandra switches to Pessimistic Concurrency Control to counter the (potentially) high abort rate. Cassandra uses per-tuple locks when it switches to Pessimistic Concurrency Control. It thus has a hybrid of Optimistic Concurrency Control and Pessimistic Concurrency Control.", 
            "support_joins": false, 
            "support_views": true, 
            "support_storagemodel": true, 
            "developer": "Apache Software Foundation", 
            "description_querycompilation": "**MARKDOWN**", 
            "description_logging": "**MARKDOWN**", 
            "support_checkpoints": true, 
            "support_storedprocedures": false, 
            "support_isolationlevels": true, 
            "system": 2, 
            "description_storagemodel": "**MARKDOWN**", 
            "description_systemarchitecture": "Cassandra has a shared-nothing architecture. Since data is partitioned across several nodes, each partition is responsible for all compute related to data on its own private shard. However, this partitioning of data across nodes in a cluster can lead to a single point of failure. Cassandra counters this by making replicas of the data.", 
            "written_in": [
                22, 
                39
            ], 
            "support_querycompilation": true, 
            "website": "http://cassandra.apache.org/", 
            "support_storagearchitecture": true, 
            "description_indexes": "Cassandra does not use one single type of index clustered on the Primary Key. Cassandra first uses a partitioner to map the key to a node in the cluster. Then it uses bloom filters to exclude some of the SSTables. This is followed by passing the key through a partition index per SSTable. It then goes through a compression index, and finally, Cassandra searches for the key within the sorted keys present in the SSTable. Furthermore, rows within a partition can be indexed, when the partition is above a certain size. Thus, there is no one right answer as to which index is used by Cassandra. All of the above mentioned data structures are either implemented as Concurrent Hash Maps, Concurrent Skip Lists, or Bit-Maps.", 
            "description": "Apache Cassandra is a free and open source NoSQL distributed database management system. It provides high availability, high scalability, and fault tolerance, even while running on commodity hardware. Cassandra has no single-point of failure, and is well-suited for clusters spanning multiple datacenter. Cassandra is decentralized and master-less. Therefore, data in a Cassandra cluster is distributed/partitioned across multiple nodes in the cluster. It provides fault tolerance by asynchronously replicating data across different nodes. Thus, Cassandra is immune to even whole data centers going down. One salient feature of Cassandra is it\u2019s ability to scale linearly in both read and write throughput as the number of nodes grows.", 
            "description_storedprocedures": "Cassandra doesn\u2019t have stored procedures. Instead, developers are expected to write their business logic in application level code, and communicate with Cassandra using a client driver to read and write data.", 
            "description_views": "**MARKDOWN**", 
            "description_queryinterface": "Cassandra has its own client interface, called the Cassandra Query Language (CQL), which closely resembles SQL. It is designed to be a simple client interface that hides and abstracts away all the complexities of Cassandra. Client drivers are present for Java (JDBC), Python (DBAPI2), Node.JS (Helenus), Go (gocql) and C++, and use the Cassandra Binary Protocol (wire protocol) to communicate with Cassandra.", 
            "description_queryexecution": "**MARKDOWN**", 
            "support_indexes": true, 
            "support_queryexecution": true, 
            "description_datamodel": "Cassandra\u2019s data model revolves around efficiently storing and retrieving key-value pairs. Each row is identified by a unique row key, and has key-value pairs for the columns (identified by name, value, and timestamp) within that row. Rows are grouped together to form \u201ccolumn families\u201d. Column families are synonymous to tables in a relational database management system. In fact, since CQL 3, column families are also called tables. The first component of a table\u2019s primary key is called the \u201cpartition key\u201d. Since Cassandra partitions tables across multiple nodes, the rows within a cluster can be conveniently clustered by the remaining columns. Thus, a table in Cassandra can be considered to be a distributed multi-dimensional map. One interesting feature of Cassandra is that not all rows within a table need to have values for all columns. Furthermore, columns can be added to any number of rows within a table (unlike a relational database where each row must have values for all columns, and adding a new column results in a new column for all rows).", 
            "support_concurrencycontrol": true, 
            "description_foreignkeys": "Not supported", 
            "description_storagearchitecture": "Cassandra is a purely disk-based database system. However, since its storage structured is similar to a log structured merge tree, it may buffer some data in a memory buffer (until it has enough data to perform a log write) before flushing it out to disk.", 
            "name": "Cassandra", 
            "license": [
                2
            ], 
            "created": "2016-07-04 19:37:30", 
            "oses": [
                9, 
                11, 
                16
            ], 
            "description_joins": "Not supported", 
            "support_queryinterface": true, 
            "description_isolationlevels": "Although Cassandra does not offer RDBMS-styled ACID transactions, it does offer \u201clight-weight transactions\u201d on a per-row basis that are implemented as simple \u201ccompare-and-set\u201d operations through Paxos. In all cases, there is only a single lock to acquire, and hence deadlock is not an issue in Cassandra. Moreover, since there are no multi-row transactions, the light-weight transactions that Cassandra does offer, offer Serializable Isolation Level.", 
            "support_foreignkeys": false, 
            "history": "Cassandra was started as a project at Facebook, and was later open-sourced in 2008 as a Google Code project. It finally became an Apache Project in 2009-2010. The database has been named after the Greek mythological prophet Cassandra."
        }
    }, 
    {
        "pk": 3, 
        "model": "systems.SystemVersion", 
        "fields": {
            "support_systemarchitecture": true, 
            "support_logging": true, 
            "creator": "yangz4@andrew.cmu.edu", 
            "description_checkpoints": "HBase Snapshots allows the users to make checkpoints of a table with little impact on RegionServers. Creating snapshots does not block reads and writes, but for each table only one snapshot can be created at a time. ", 
            "support_datamodel": true, 
            "description_concurrencycontrol": "HBase guarantees ACID semantics per-row. HBase uses a form of Multiversion Concurrency Control (MVCC) to avoid row locks for read operations. Write operations still need to acquire row locks. ", 
            "support_joins": false, 
            "support_views": false, 
            "support_storagemodel": true, 
            "developer": "http://hbase.apache.org/team-list.html", 
            "description_querycompilation": "", 
            "description_logging": "HBase's write-ahead-log is named HLog. ", 
            "support_checkpoints": true, 
            "support_storedprocedures": false, 
            "support_isolationlevels": true, 
            "system": 3, 
            "description_storagemodel": "HBase is schema-less column-oriented datastore. ", 
            "description_systemarchitecture": "HBase is organized as a cluster of HBase nodes that complies with the master-slave architecture. There are two types of nodes: a master node, and one or more slave nodes called RegionServers. RegionServers serve data for reads and writes. An HBase Region is a subset of an HBase table that has a continuous range of sorted rowkeys. Region assignment and DDL operations are handled by the HBase Master. HBase is built on top of Hadoop. The Hadoop DataNodes store the data that RegionServers are managing. The HDFS Zookeeper maintains the server status in the cluster. ", 
            "written_in": [
                22
            ], 
            "support_querycompilation": false, 
            "website": "http://hbase.apache.org/", 
            "support_storagearchitecture": true, 
            "description_indexes": "For each table, HBase only provide an B+Tree like index on row keys. HBase does not natively support secondary indexes. Users can use filters for querying on non-rowkey columns. There are some techniques to create another table which can be used as a secondary index. ", 
            "description": "**HBase** is an open source, distributed, non-relational, scalable big data store that runs on top of Hadoop Distributed Filesystem. Hbase is suitable for storing large quantities of data, but it lacks many of the features that relational database management systems usually have, such as column types, secondary indexes, advanced query languages, etc. HBase stores the data in rows and columns. A row is referenced by a row key, and columns are grouped into \"column families\". HBase is written in Java, and is supported by Apache Software Foundation.", 
            "description_storedprocedures": "Stored procedures are not directly supported in HBase. But users can use coprocessors to resemble store procedures. ", 
            "description_views": "HBase does not provide views. But users can write MapReduce programs to approximate views. Apache Phoenix, a SQL interface for HBase, provides support for views. ", 
            "description_queryinterface": "HBase does not provide native support for SQL. Unlike RDBMS, HBase has four primary operations: Get, Put, Scan and Delete. It also has some DDL operations, e.g., Create. HBase provides a shell which users can fire queries from. Users can specify table name, column names and apply filters in their query. HBase also offers Java Client API and Thrift/REST API. Some third-party drivers are also available for other programming languages. ", 
            "description_queryexecution": "", 
            "support_indexes": true, 
            "support_queryexecution": true, 
            "description_datamodel": "An HBase table consists of rows and columns. Rows are referenced by row keys which are raw byte arrays and are sorted by row key. The sort is byte-ordered. Each row contains columns. A column's content is also an uninterpreted array of bytes. All columns belong to a column family. ", 
            "support_concurrencycontrol": true, 
            "description_foreignkeys": "HBase does not directly support refential integrity. Users can use a coprocessor to enforce foreign keys. ", 
            "description_storagearchitecture": "HBase leverages HDFS as the backend storage. Currently HDFS is disk-oriented. ", 
            "name": "HBase", 
            "license": [
                2
            ], 
            "created": "2016-07-04 19:37:30", 
            "oses": [
                2
            ], 
            "description_joins": "HBase does not support join operations. Users can implement joins in their application code. ", 
            "support_queryinterface": true, 
            "description_isolationlevels": "HBase only provide \"read committed\" isolation level. Users can downgrade the isolation level to \"read uncommitted\" by modifying the source code. ", 
            "support_foreignkeys": false, 
            "history": "HBase was initially a project by the company Powerset, a San Francisco-based search and natural language company. Microsoft acquired Powerset in 2008."
        }
    }, 
    {
        "pk": 4, 
        "model": "systems.SystemVersion", 
        "fields": {
            "support_systemarchitecture": true, 
            "support_logging": true, 
            "creator": "haibin@cmu.edu", 
            "description_checkpoints": "MemSQL uses multi-version concurrency control and it's natural to create a consistent(non-fuzzy) snapshot without the need to block ongoing transactions. ", 
            "support_datamodel": true, 
            "description_concurrencycontrol": "MemSQL uses multi-version concurrency control. Reads are not blocked, writes acquire row-level locks. ", 
            "support_joins": true, 
            "support_views": true, 
            "support_storagemodel": true, 
            "developer": "MemSQL Inc", 
            "description_querycompilation": "Instead of the traditional interpreter based execution model, MemSQL 5 comes with a new code generation architecture, which compiles a SQL query to LLVM to machine code.\n When the MemSQL server encounters a SQL query, it parses SQL into AST and extracts parameters from the query, which is then transformed into a MemSQL-specific intermediate representation in *MemSQL Plan Language*(MPL). MemSQL then flattens MPL AST into a more compact format as *MemSQL Bytecode*(MBC). Plans in MBC format is then transformed into LLVM Bitcode, which LLVM uses to generate machine code. \n Such code generation architecture enables many low-level optimizations and avoids much less unnecessary work compared to interpreter-based execution. Compiled plans are also cached on disk for future use. ", 
            "description_logging": "MemSQL implements write-ahead-logging which records only committed transactions. It uses a transaction buffer pool as back-pressure mechanism so that a worker thread doesn't generate indefinite amount of logs. Replication is implemented based on log recovery. ", 
            "support_checkpoints": true, 
            "support_storedprocedures": false, 
            "support_isolationlevels": true, 
            "system": 4, 
            "description_storagemodel": "In MemSQL, row segments in rowstore are stored in N-ary storage model in-memory. Column segments in columnstore are stored in decomposition storage model. Clustered columnar indexes are created for columnstore and compression is applied. ", 
            "description_systemarchitecture": "MemSQL has a two-tier, clustered architecture. The nodes in upper tier are **aggregators**, which are cluster-aware query routers. One special node called **Master Aggregator** is responsible for clustering monitoring. The nodes in lower tier are **leaves**, which store and process partitioned shards. The aggregator sends extended SQL to leaves to perform distributed query execution. ![](https://www.filepicker.io/api/file/9KGdXi6RQZua6ubrb8qB)", 
            "written_in": [
                7
            ], 
            "support_querycompilation": true, 
            "website": "http://www.memsql.com/", 
            "support_storagearchitecture": true, 
            "description_indexes": "Skip list is the default index type in MemSQL. Skip list is lock free and thus leads extremely fast insert performance, and O(lg(n)) expected lookup/insert/delete performance. Unlike B+ tree, skip list is singly linked, thus reserve scan leads to twice as costly as forward scan. \n Skip list involves more pointer chasing than B+ tree which could potentially lead to more cache misses. In MemSQL, heuristics are applied to organize nearby nodes on the same physical page to mitigate penalties caused by pointer chasing. \n Lock-free hash table is also supported in MemSQL to perform fast exact-match queries. ", 
            "description": "MemSQL is a distributed in-memory relational database with high performance on both transactional and analytical workload, well-integrated with Spark & Kafka for real-time analysis.", 
            "description_storedprocedures": "Stored procedure is not supported. ", 
            "description_views": "Views in MemSQL is not materialzed and cannot be written into.", 
            "description_queryinterface": "MemSQL supports a subset of MySQL syntax, plus extensions for distributed SQL, geospatial and JSON queries. MySQL wire protocol is supported. ", 
            "description_queryexecution": "MemSQL uses Tuple-at-a-Time Model for rowstore query execution, uses Vectorized Model for columnstore.", 
            "support_indexes": true, 
            "support_queryexecution": true, 
            "description_datamodel": "MemSQL is a distributed relational database. It also supports two-column key/value store.", 
            "support_concurrencycontrol": true, 
            "description_foreignkeys": "MemSQL currently supports foreign keys to assist sharding, but referential integrity is not enforced. ", 
            "description_storagearchitecture": "In MemSQL, rowstore is completely in-memory and columnstore is disk-backed. ", 
            "name": "MemSQL", 
            "license": [
                12
            ], 
            "created": "2016-07-04 19:37:30", 
            "oses": [
                9
            ], 
            "description_joins": "Nested loop join, index-nested loop join, merge join and hash join are supported in MemSQL. Joins between two Columnstore tables are often executed as sort merge join. \n For distributed join queries, if two tables are joined with identical shard key, the join will be performed locally; otherwise dataset is broadcast to other nodes via the network. ", 
            "support_queryinterface": true, 
            "description_isolationlevels": "MemSQL supports read committed isolation level.", 
            "support_foreignkeys": false, 
            "history": "MemSQL was a Y-combinator graduate, founded in 2011."
        }
    }, 
    {
        "pk": 5, 
        "model": "systems.SystemVersion", 
        "fields": {
            "support_logging": true, 
            "creator": "wendongl@andrew.cmu.edu", 
            "description_checkpoints": "In BigTable, SSTables are immutable and persistent in GFS. Therefore, only the writes to memtable will generate logs. \nAlthough BigTable does not do checkpointing explicitly, it has something that is in effect doing a checkpoint: When a memtable gets too large, the system will do a compaction on it and transform it into an SSTable[1]. This is effectively a checkpointing on this memtable.", 
            "description_concurrencycontrol": "BigTable only supports transactions on a single row[1]. It does not support transactions spanning multiple rows", 
            "support_joins": false, 
            "support_views": false, 
            "support_storagemodel": true, 
            "developer": "Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, Robert E. Gruber", 
            "description_logging": "BigTable uses physical logging. For performance consideration, all tablets on a tablet server write logs to the same log file[1]. ", 
            "support_checkpoints": false, 
            "support_storedprocedures": false, 
            "support_isolationlevels": false, 
            "system": 5, 
            "description_storagemodel": "In BigTable, a table is split into multiple tablets, each of which is a subset of consecutive rows[1]. A tablet is a unit of data distribution and load balancing. Different tablets of a table may be assigned to different tablet servers. A tablet is stored in the form of a log-structured merge tree[2] (which they call memtable and SSTable). \n\nFurthermore, BigTable allows clients to create locality group[3]. A locality group is a subset of columns in a table. BigTable will create a separate SSTable for each locality group, which will improve read performance of this locality group.", 
            "support_datamodel": true, 
            "written_in": [
                5, 
                7
            ], 
            "support_querycompilation": false, 
            "website": "https://cloud.google.com/bigtable/", 
            "support_storagearchitecture": true, 
            "description": "BigTable[1] is a distributed storage system used in Google, it can be classified as a non-relational database system. BigTable is designed mainly for scalability. It typically works on petabytes of data spread across thousands of machines. \n\nThere is not much public information about the detail of BigTable, since it is proprietory to Google. The most authoritative information about it is its paper[1]. An open source implementation of it based on its original paper is Apache HBase[2]. \n\nGoogle has now provided BigTable as its cloud NoSQL database service[3]. The documentation of that[4] might be helpful, too.", 
            "description_queryinterface": "BigTable provides clients with the following APIs: \n1. Look Up (Read a Single Row) \n2. Scan (Read a subset of rows) \n3. Write \n4. Delete \n5. Customized Scripts (written in Sawzall language)", 
            "support_indexes": false, 
            "description_datamodel": "BigTable does not support relational data model. Instead, it provides users the ability to create column families in a table. \n\nEach table usually contains a small number of column families, which should be rarely changed (because the change of them involves metadata change). Inside each column family, there can be unlimited number of columns. Users can freely add or delete columns in a column family. Deleting of an entire column family is also supported. \n\nBigTable does not have any type information associated with a given column. It only treats data as strings of bytes.", 
            "support_concurrencycontrol": false, 
            "description_storagearchitecture": "BigTable assumes an underlying reliable distributed file system (here is Google File System). The tablets are stored in Google File System, which is a disk-oriented file system. \nThe most recently written records are stored in memtable, which is in memory. However, most of the data is stored on disk.", 
            "name": "BigTable", 
            "license": [
                12
            ], 
            "created": "2016-07-04 19:37:30", 
            "oses": [
                9
            ], 
            "support_queryinterface": true, 
            "support_foreignkeys": false, 
            "history": "BigTable was among the early attempts Google made to manage big data. Jeffrey Dean and Sanjay Ghemawat were involved in it. It is one of the three components Google built for managing big data (the other two are Google File System[1] and MapReduce[2]). \n\nThese three components focus on different aspects of big data: Google File System is a reliable distributed file system that the other two build upon; MapReduce is a distributed data processing framework; BigTable is a distributed storage system. \n\nThese three projects are very famous in distributed system. They all have their open source implementation.[3][4][5]"
        }
    }, 
    {
        "pk": 6, 
        "model": "systems.SystemVersion", 
        "fields": {
            "support_systemarchitecture": true, 
            "support_logging": true, 
            "creator": "dxiao1@andrew.cmu.edu", 
            "description_checkpoints": "", 
            "support_datamodel": true, 
            "description_concurrencycontrol": "Transactions in CockroachDB is implemented ", 
            "support_joins": true, 
            "support_views": false, 
            "support_storagemodel": true, 
            "developer": "[Contributors](https://github.com/cockroachdb/cockroach/graphs/contributors)", 
            "description_querycompilation": "", 
            "description_logging": "There's no logging in CockroachDB because it adopts MVCC and uses transaction table to control transactions.", 
            "support_checkpoints": false, 
            "support_storedprocedures": false, 
            "support_isolationlevels": true, 
            "system": 6, 
            "description_storagemodel": "CockroachDB stores its data in the distributed key-value store.", 
            "description_systemarchitecture": "CockroachDB has two layers, the SQL layer and the storage layer. The SQL layer sits on top of the transactional and strongly-consistent distributed key-value store. In the key-value store, the key ranges are divided and stored in RocksDB and replicated across cluster. It exports structured data API of relational concepts. It exports standard SQL interface at the SQL layer. The SQL layer translates SQL statements into calls to structured data API.", 
            "written_in": [
                19
            ], 
            "support_querycompilation": false, 
            "website": "https://www.cockroachlabs.com/", 
            "support_storagearchitecture": true, 
            "description_indexes": "CockroachDB supports primary key, secondary index. The indexes are implemented with the distributed key-value store ranges.", 
            "description": "CockroachDB is a scalable, fault-tolerant, SQL database built on a transactional and strongly-consistent key-value store. It is backed by RocksDB and uses distributed consensus algorithm to ensure consistency, it is inspired by Spanner wait commit to implement serializable. It is currently in beta. (Because CockroachDB is rapidly changing, so many findings are based on design document, outdated documentation or available code.)", 
            "description_storedprocedures": "", 
            "description_views": "", 
            "description_queryinterface": "CockroachDB aims to provide standard SQL with extensions, but some standard SQL functionality is under development.", 
            "description_queryexecution": "The query execution is similar to stream processing. When executing a query, the execution plan is transformed into a DAG, a gateway node orchestrates participating nodes begin processing.", 
            "support_indexes": true, 
            "support_queryexecution": true, 
            "description_datamodel": "", 
            "support_concurrencycontrol": true, 
            "description_foreignkeys": "The design for foreign key is under discussion.", 
            "description_storagearchitecture": "The backend storage in CockroachDB is RocksDB. It is disk-oriented storage.", 
            "name": "CockroachDB", 
            "license": [
                2
            ], 
            "created": "2016-07-04 19:37:30", 
            "oses": [
                9, 
                11, 
                16
            ], 
            "description_joins": "Because CockroachDB is backed by key value store. There are two ways of join in CockroachDB, join-by-lookup and stream joins. Join-by-lookup iterate through one table and lookup for it in the other table. Stream joins is an execution plan based on hashing.", 
            "support_queryinterface": true, 
            "description_isolationlevels": "CockroachDB supports snapshot isolation (SI) and serializable snapshot isolation (SSI). It is implemented using RocksDB snapshot capability.", 
            "support_foreignkeys": false, 
            "history": "CockroachDB is inspired by Google\u2019s Spanner and F1 to address fault tolerance and distributed consistency."
        }
    }, 
    {
        "pk": 7, 
        "model": "systems.SystemVersion", 
        "fields": {
            "support_systemarchitecture": true, 
            "support_logging": true, 
            "creator": "ssanturk@andrew.cmu.edu", 
            "description_checkpoints": "Checkpointing involves flushing in memory buffers to disk to reduce recovery time, which might require the transactions to be blocked or aborted. BerkeleyDB selects the active transaction with the lowest Log Sequence Numbers (LSN) and flushes pages corresponding to that LSN to the disk. A checkpoint record is then updated to indicate the highest LSN that has been checkpointed so far. Checkpointing frequencies are tunable to help users amortize these costs.", 
            "support_datamodel": true, 
            "description_concurrencycontrol": "BerkleyDB uses two-phase locking to permit multiple reader cursors or a single writer cursor to access the database. Initial versions of BerkeleyDB only supported monolithic, table-level locking. To prevent deadlocks, BerkeleyDB uses creates a conflict matrix by default and grants and denies lock requests by inspecting this table first. The conflict matrix supports hierarchical lock requests. Recent releases now support MVCC as well.", 
            "support_joins": true, 
            "support_views": false, 
            "support_storagemodel": true, 
            "developer": "Sleepycat Software and Oracle", 
            "description_querycompilation": "Sqlite over BerkeleyDB uses code generation to produce virtual machine code. The virtual machine acts as the bridge between sqlite and the storage layer (in this case, BerkeleyDB) and has the logic to translate virtual machine code to storage level calls.", 
            "description_logging": "BerkeleyDB supports Write Ahead Logging (WAL) and uses this for durability instead of immediately persisting every transaction onto the disk. The logs follow append-only semantics and are indexed using Log Sequence Numbers (LSN). The logging in BerkeleyDB follows the ARIES model by providing undo and redo logs facilitated by LSN indexing. Logs are coupled with additional metadata indicating the expected size of the record to be returned. MPools can be evicted only when the data that they are holding is persisted on stable storage. This is validated by the log manager by using LSNs. The Log manager can read records from the disk using these LSN index to compute the offset of data on the storage layer and seek to that position. Logs are forcibly persisted to the disk as soon as they are generated. ", 
            "support_checkpoints": true, 
            "support_storedprocedures": false, 
            "support_isolationlevels": true, 
            "system": 7, 
            "description_storagemodel": "BerkeleyDB is a simple KeyValue store. Data is stored as raw-bytes. Key-value pairs are stored in \"DTAG\" that store a pointer to the memory location of the byte string and its size (along with other bookkeeping data). These structures allow the data to be of unbounded length. The individual DTAG storage pattern depends on the access method. However, this model is susceptible to pointer-chasing slow downs in high-performance systems.", 
            "description_systemarchitecture": "BerkeleyDB is an embedded storage library. Hence, it's scope is within the process space of the process that created it. This implies that every possible hardware resource is shared. There are no special hardware requirements for BerkeleyDB. The Replicated, high availability mode uses the shared nothing system architecture.", 
            "written_in": [
                1, 
                5, 
                6, 
                7, 
                8, 
                12, 
                14, 
                15, 
                16, 
                19, 
                21, 
                22, 
                24, 
                26, 
                27, 
                28, 
                29, 
                32, 
                34, 
                35, 
                38, 
                39, 
                40, 
                41, 
                42, 
                43, 
                44, 
                46, 
                47, 
                49
            ], 
            "support_querycompilation": true, 
            "website": "http://www.oracle.com/technetwork/database/database-technologies/berkeleydb/overview/index.html", 
            "support_storagearchitecture": true, 
            "description_indexes": "Due to the simple key-value format of BerkeleyDB records, bot keys and values are directly stored in the leaves of the B+Tree. The B+Tree index is sorted by keys and offers efficient exact match lookups and range scans. Prefix compressions isn't used in B+Trees. The hash index is used to support exact-match lookups and store data as exact key-value mappings. Hash indexes use linear hashing to balance the keys across buckets. BerkeleyDB also provides recno indexes that are built on top of B+Trees for ordering and storing data sequentially.", 
            "description": "BerkeleyDB is a simple and flexible, embedded open-source, database storage library. The simplicity arises from the fact that it is a basic key-value store and not a full-fledged database system that provides querying and schema constraints. It offers flexibility by being schema-less and by providing convenient mechanisms to include and discard features. BerkeleyDB provides complete ACID compliance through its five subsystems, namely, caching, datastore, locking, logging and recovery. All these five subsystems are implemented using concepts like two-phase locking, undo/redo write-ahead logging, etc. This enables it to offer high performance for a variety of workloads and handle very large-sized key-vale stores (order of Terabytes). Due to these properties, many applications like filesystems, LDAP servers and database systems (like MySQL) use BerkeleyDB for their backend storage requirements.", 
            "description_storedprocedures": "BerkeleyDB inherits the relational database processing functionality from Sqlite and Sqlite doesn't support stored procedures.", 
            "description_views": "Since BerkeleyDB is not a relational database, it doesn't have a concept of views.", 
            "description_queryinterface": "The basic Query interface support for BerkeleyDB is the simple CRUD (Create, Read Update and Delete) API, which, are the common primitives provided for key-value stores. This API has been implemented in most common programming languages. Recent releases from Oracle have provided SQL support by providing the SQLite API that parses SQL queries and generates code that can be executed on BerkeleyDB. Third party support for PL/SQL is available.", 
            "description_queryexecution": "BerkeleyDB, on its own, does not support query execution. BerkeleyDB uses Sqlite to provide an API to execute SQL queries and Sqlite uses the Tuple-at-a-Time query execution model.", 
            "support_indexes": true, 
            "support_queryexecution": true, 
            "description_datamodel": "BerkeleyDB is primarily a key-value store. The keys follow a uniqueness constraint. It has support for variable-length keys and values for BTree and Hash index access methods. The Queue access method supports only fixed-sized values. The DB can be configured to provide multi-value support for keys. The BerkleyDB XML repository provides an XML document store that is backed by XQuery.", 
            "support_concurrencycontrol": true, 
            "description_foreignkeys": "Foreign key constraints are supported that allow foreign key deletions to be either Abort, Cascade or Nullify.", 
            "description_storagearchitecture": "BerkeleyDB is fully disk-oriented and uses buffer pools called \"Mpool\" in memory. Page format on-disk and in memory remains the same to trade off the format conversion overhead. Mpool management follows and LRU page replacement policy with pinning for dirty pages. The MPool is designed to provide a complete in-memory abstraction over the disk storage. ", 
            "name": "BerkeleyDB", 
            "license": [
                1, 
                3
            ], 
            "created": "2016-07-04 19:37:30", 
            "oses": [
                1, 
                2, 
                4, 
                5, 
                6, 
                7, 
                8, 
                9, 
                10, 
                11, 
                13, 
                14, 
                15, 
                16, 
                18, 
                20
            ], 
            "description_joins": "Joins supported by BerkeleyDB depend on the joins supported by Sqlite. Sqlite has recently introduced support for merge joins, but aren't complete enough to support joins over non-unique keys. That said, BerkeleyDB can only support unique keys which makes the available sort-merge functionality sufficient.", 
            "support_queryinterface": true, 
            "description_isolationlevels": "BerkeleyDB provides serializable isolation by default when pessimistic concurrency control is used. This is possible due to the multiple reader-single writer model and two phase locking. When MVCC is used, snapshot-level isolation guarantees are provided. The caveat is, that this isolation guarantee can work well only if the cache sizes are large. MVCC leads to the creation of multiple working sets and tend to grow larger than the cache size.", 
            "support_foreignkeys": true, 
            "history": "*BerkeleyDB originally started off to replace the existing decoupled in-memory and on-disk hashing systems with a single unified system that caches an on-disk table into memory and uses in-memory buffer pooling to provide an illusion of infinite memory availability. This library used linear hashing to efficiently mp keys to values for constant time lookups and store a large volume of data backed by the disk. This was extended to include the BTree access method and then followed up by a simple CRUD API that provides call agnostic to the access method deployed. The initial releases developed by Keith Bostic and Margo Seltzer (who were graduate students at the time) were upgraded to provide complete transactional processing, through the sponsorship of Netscape for developing a LDAP server. The developers then began to create releases as Sleepycat Software which finally got acquired by Oracle in 2006. Presently Oracle presently develops and maintains releases and permits free and open source usage bounded by the GNU AGPL license terms. "
        }
    }
]